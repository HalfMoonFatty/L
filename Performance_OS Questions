performance:


+ CPU

  vmstat - system CPU utilization
  mpstat - process CPU utilization
  prstat - who is using the CPU



+ Memory

  System Memory: vmstat -p

  Process Memmory: 
    Process Virtual and Resident Set Size – ps –eo pid,vsz,rss,args
    Using pmap to Inspect Process Memory Usage – pmap –x pid

  Swap: 
    Swap Summary: swap –s
    iostat to determine if the physical swap devices are currently busy with I/O



+ Disk

Writing:
  writeback mode: the completion interrupt issent as soon as the cache receives the data. 
  writethrough mode, writes now suffer a delay as the storage array waits for them to write to disk, before an I/O completion is sent. 

Reading:
  The storage array tries its best to serve reads from (its very large) cache,especially effective if prefetch is enabled and the workload is sequential. 

IO Size:
  For sequential access, larger I/O sizes are better; 
  For random access, I/O sizes should to be picked to match the workload. First need to know your workload.

iostat:
  3 Starting points of disk behavior: Utilization Saturation and Tput
  %b: this is percent busy and tells us disk utilization.
  wait: the average wait queue length; it is a measure of disk saturation. 
  kr/s and kw/s: kilobytes read and written per second, which tells us the current disk throughput.



+ Network

  • Packets. netstat –i Network interface packet counts.
  • Bytes. Kstat, SNMP, nx.se, and nicstat Measuring throughput in terms of bytes/sec.
  • Utilization. nicstat calculates utilization = current throughput / known maximum.
  • Saturation. network applications usually experience delays.
  • Errors. netstat -i to print error counts: collisions (small numbers are normal), input errors, and output errors (late collisions).


  netstat : packet counts and error counts
  nicstat Tool: network utilization and saturation by interface.
  ping Tool – host up and resp time
  traceroute Tool - discover the hops to a host (print response time to each hop)



+ Process
  ps
  prstat
  • SIZE. The total virtual memory size of mappings within the process, including all mapped files and devices.
  • RSS. Resident set size. The amount of physical memory mapped into the process.
  • TIME. The cumulative execution time for the process, printed in CPU hours, minutes, and seconds.
  • CPU. The percentage of recent CPU time used by the process.
  • PROCESS/NLWP. The name of the process (name of executed file) and the number of threads in the process.


  pstack - you can often determine where the process is spending most of its time
  truss - traces system calls made on behalf of a process.
  dtrace



+ Lockstat

  Two main types of user-level locks:
  • Mutex lock. An exclusive lock. A mutex lock attempts to spin while trying obtain the lock if the holder is running on a CPU, or blocks if the holder is not running or after trying to spin for a predetermined period.
  • Reader/Writer Lock. A shared reader lock. Only one person can hold the write lock, but many people could hold a reader lock while there are no writers.



+ CPU Caches

size, cache line size and  set-associativity.
• A greater size improves cache hit ratio;
• A larger cache line size can improve throughput;
• A higher set-associativity improves the effect of the Least Recently Used policy, which can avoid hot spots where the cache would otherwise have flushed frequently accessed data.









linkedin的页面load很慢，可能是哪些原因导致的？可以有什么办法来衡量，可以怎么解决etc

Start by drawing the system architecture diagram: client, server, database.
Then mention that there are many places things can go wrong.

Client:
- Most browsers support inspecting network traffic. In Chrome, this is right click -> "Inspect elements" -> "Network" tab.
  It shows the RTT and latency breakdown for every HTTP call issued to render the page. From there, you can find some
  performance bottlenecks, e.g. if a particular server is slow, or if some requested file's size is too large.
- If some requested file's size is too large, we can consider compression. Multimedia files (images, audios, videos) are good
  candidates. Sometimes, when latency is really critical, we can also consider to compress text files like JS files. There are
  open-source JS "compilers" that will "compress" JS file by doing tricks like renaming long var names to shorter ones (e.g.
  "var total_length = 0;" becomes "var t=0;").
- The JS code itself may be inefficient. For example, some component may be issuing synchrounous calls to get content, which
  blocks subsequent calls. It's better to issue AJAX calls instead to unblock subsequent calls.s

Server:
- Server serves each request by a request handler running in a dedicated thread. Threads come from a pre-allocated thread-pool
  that the server owns and constructs at start time. If the thread-pool is fully occupied, the new request threads will be
  blocked until a new thread is available. So there are several options to optimize on this direction:
  *  Increase the number of threads in thread pool. This is the simplest solution, and often works. The drawback is that having
     too many *idle* threads in the pool may actually degrade performance.
  *  Investigate why the pending threads are taking so long. Sometimes there may even be a bug in the server that never
     terminates a request thread, which makes that thread resource forever leaked. In this case, unless the server restarts,
     that thread is forever gone. So if we don't restart the server for long enough, and the bug happens often enough, we may
     risk leaking *all* theads in the thread pool, and suddenly all subsequent requests will timeout!
     - Obviously we should try to fix the bug if we can. If we can't identify the bug, at lease we should have monitors on the
       number of active requests, so oncall can restart the server in case this happens.
- Increase the replication factor. If we can afford more physical resources, it's better to repliate more servers, rather than
  increase the number of threads in the thread pool. Web application servers typically serves HTTP requests, which by definition
  is stateless, so we don't need to worry about consistency for replicating the web servers.
- If disk IO is taking too long, we can consider to add memcache servers sitting next to our application servers. Network delays
  are typically smaller than disk IO delays. Also note that memcache can be deployed with replication, and memcache library
  guarantees consistency.

Database:
- If the database queries are slow, we can investigate the SQL queries sent from our application server. The first thing to
  do is to run "EXPLAIN" on your SQL (e.g. "EXPLAIN SELECT * FROM Foo WHERE Id = 123;"). It shows you the query execution plan,
  including the DB indexes used and whether it is a full table scan.
- From there, you can brainstore possible optimizations. Maybe we can get rid of some unecessary joins, apply additional
  filters, or break our SQL into multiple SQLs and request in parallel (there are many more SQL optimization possibilities
  that's case-by-case, so this is really an open-discussion).
- We can also consider to introduce index on some table to speed up our query. Again this is based on the query plan.
- If we have too much data stored in a single-server database, even with index the SQL may still be slow. In this case we may
  consider using distributed database with data sharding. We can either do horizontal or vertical data sharding. This allows
  higher degree of parallelism, but may incur additional cost of cross machine traffic when the requested data is spread across
  multiple machines.
- At some point, RDBMS will be too slow even under a distributed settings. There is this famous CAP theorem which says you can
  only get two out of {consistency, availability, partition-tolerance}. "P" is usually a must-have (e.g. if there is suddenly a
  network cut through the middle of US, we should still be able to serve both people from NY and people from SF). Distributed
  RDBMS sacrafices "A" to ensure "C", because RDBMS must honor ACID semantics. OTOH, if our application is ok with eventual
  consistency, we can consider to use a NoSQL system. FYI, in contrast to ACID, people often say NoSQL guarantees BASE semantics,
  i.e. "Basically-Available, Soft-State". This is just a fancy way of saying the system is eventual-consistent, but not
  guaranteed for availability.


==

How to generate a performance report for a given URL? What to save, how to represent the result?

This is a design question for a monitoring system. At a high level, the end user is just interested in various performance
metrics such as the latencies to load different resources for the given URL. But this is actuall very easy to obain from
browser -- right click page --> inspect --> network tab.

So the interviewer is likely looking for deeper designs, i.e. design a monitoring system: https://docs.google.com/document/d/1PKkYvkfoAuH0I9NIM9F3Q3zDzANcxOnikn9_dXNvWnc/edit#heading=h.fs74lgjlidt1.
Take a few minutes to read through that design first (you may actually be asked about this same question, since the interview
module for performane SWE covers monitoring).

Specifically for this problem, you can first say that report has a tabular strucutre. And for any report, whether is for
performance analysis or for other cases like financial analysis, they all share the common concept of dimensions and metrics.
Dimensions are things that don't change much, like IDs, names, dates, etc. Metrics are things that are more dynamic, e.g.
latencies, various counts, etc.

For URL performance report, you can first confirm the set of metrics user would like to see in the report, e.g.:

- latencies
- request size
- response size

And the set of dimensions:

- URL
- Sender ID (e.g. IP address)
- Response status (e.g. 200 for OK, 404 for NOT FOUND, 302 for REDIRECT, etc.)
- Resource type (e.g. document, gif, jpeg, script, etc.)

User can define custom subset of dimensions and/or metrics for the repoert, and your reporting system can do aggregations.
e.g. If user only wants a report with (URL, latency), then your system would take the average latencies for all recorded
entries. More concretely, if your recorded values are:

URL      IP      Status      Type      Latencies      Request Size      Response Size
a.com    1.2.3.4 200         gif       100ms          1KB               2KB
a.com    1.2.3.5 404         script    200ms          2KB               1KB

Then the report is:

URL      Latencies
a.com    150ms

as you aggregate over latencies using the AVG aggregator.

Next lets move on to how to design the monitoring system. As the design doc shows, you can embed a monitoring library to client
code to record these statistics. For URL statistics, the client is the browser, so the monitoring library has to be in
javascript. On the monitoring system backend, you can have a server to record these monitored metrics, and store them in an
OLAP database, which supports fast aggregation queries.

==

process/thread区别，thread如何安全访问共享内存->mutex，

Both processes and threads are independent sequences of execution. 
The typical difference is that threads (of the same process) run in a shared memory space, 
while processes run in separate memory spaces.


==

Race condition:

A race condition occurs when two or more threads can access shared data and they try to change it at the same time. 
Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the 
threads will attempt to access the shared data. Therefore, the result of the change in data is dependent on the thread 
scheduling algorithm, i.e. both threads are "racing" to access/change the data.


==

semaphone/mutex区别


Strictly speaking, a mutex is locking mechanism used to synchronize access to a resource. 
Only one task (can be a thread or process based on OS abstraction) can acquire the mutex. 
It means there is ownership associated with mutex, and only the owner can release the lock (mutex).

Mutexes are typically used to serialise access to a section of  re-entrant code that cannot be executed 
concurrently by more than one thread. A mutex object only allows one thread into a controlled section, 
forcing other threads which attempt to gain access to that section to wait until the first thread has exited from that section.

--------

Semaphore is signaling mechanism (“I am done, you can carry on” kind of signal). 

A semaphore restricts the number of simultaneous users of a shared resource up to a maximum number. 
Threads can request access to the resource (decrementing the semaphore), and can signal that they have 
finished using the resource (incrementing the semaphore).


==

Deadlock

Deadlock describes a situation where two or more threads are blocked forever, waiting for each other. 

4 Conditions for deadlock:

Mutual Exclusion
Hold and Wait
Circular Wait
No Preemption

==

Virtual Memory

It maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory. 
Main storage, as seen by a process or task, appears as a contiguous address space or collection of contiguous segments. 
The operating system manages virtual address spaces and the assignment of real memory to virtual memory. 

Address translation hardware in the CPU, often referred to as a memory management unit or MMU, 
automatically translates virtual addresses to physical addresses. 


==

Trashing

Thrashing occurs on a program that works with huge data structures, as its large working set causes continual page faults 
that drastically slow down the system. Satisfying page faults may require freeing pages that will soon have to be re-read from disk. 
 
 
==

Page Fault 

An interrupt that occurs when a program requests data that is not currently in real memory. 
The interrupt triggers the operating system to fetch the data from a virtual memory and load it into RAM.


==

TLB

A translation lookaside buffer (TLB) is a memory cache that stores recent translations of 
virtual memory to physical addresses for faster retrieval.


==




